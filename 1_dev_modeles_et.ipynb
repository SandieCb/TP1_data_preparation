{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Développement de modèles de Machine Learning\n",
    "\n",
    "**Auteur:** Sandie Cabon  \n",
    "**Date:** 02 février 2026\n",
    "\n",
    "Ce notebook développe plusieurs modèles de classification pour prédire l'insuffisance cardiaque avec différentes stratégies de gestion des données manquantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import make_scorer, recall_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from helping_functions import infer_column_types, specificity_score, generate_performance_figure, identify_best_param\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration (NE PAS MODIFIER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############### CHARGEMENT DES DONNÉES #####################\")\n",
    "\n",
    "# close older figures\n",
    "plt.close(\"all\")\n",
    "\n",
    "# load heart failure dataset\n",
    "dataset = pd.read_csv(\"heart_failure_dataset.csv\")\n",
    "\n",
    "# apply good type to dataframe (custom function)\n",
    "dataset = infer_column_types(dataset)\n",
    "\n",
    "# Définition des scorers\n",
    "scorers = {\n",
    "    'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
    "    'sensibilité': make_scorer(recall_score),\n",
    "    'spécificité': make_scorer(specificity_score)\n",
    "}\n",
    "\n",
    "scorer = 'balanced_accuracy'\n",
    "\n",
    "random_state = 12\n",
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des variables en fonction de leurs types\n",
    "target_feat = ...\n",
    "num_feat_names = list(dataset.select_dtypes(include=np.number).columns)\n",
    "cat_feat_names = list(dataset.select_dtypes(include='category').columns)\n",
    "cat_feat_names.remove(target_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Développement Modèle 0\n",
    "\n",
    "### Stratégie : Suppression des observations avec données manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"############ III. DEVELOPPEMENT MODELE 0 ############\")\n",
    "print(\"\\n # Preparation des données #\")\n",
    "print(\"\\n --> Gestion des données manquantes\")\n",
    "\n",
    "# completer pour récupérer le jeu de données sans les observations avec une valeur manquantes\n",
    "# aide : utiliser la fonction dropna()\n",
    "clean_dataset = ...\n",
    "\n",
    "# Nombre d'observations et de variables présentes dans le jeu de données\n",
    "# aide : utiliser la fonction len()\n",
    "n_obs_new = ...\n",
    "n_var_new = ...\n",
    "\n",
    "print(f\"\\nLe jeu de données contient {n_obs_new} observations et {n_var_new} variables.\")\n",
    "\n",
    "# Séparation des variables prédictives et de la variable cible\n",
    "clean_dataset_feat = clean_dataset.drop(target_feat, axis=1)\n",
    "clean_dataset_target = clean_dataset[target_feat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline de prétraitement\n",
    "\n",
    "Compléter le pipeline permettant de mettre à l'echelle les variables numériques avec un RobustScaler()\n",
    "les variables binaire/catégorielle restent inchangées.\n",
    "\n",
    "aide 1 : https://machinelearningmastery.com/columntransformer-for-numerical-and-categorical-data/\n",
    "\n",
    "aide 2 : https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ColumnTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21200\\1888088810.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m preprocessor0 = ColumnTransformer(\n\u001b[0m\u001b[0;32m      2\u001b[0m     transformers=[\n\u001b[0;32m      3\u001b[0m         ('num', Pipeline(steps=[\n\u001b[0;32m      4\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[1;34m'scaler'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         ]), num_feat_names),\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ColumnTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "preprocessor0 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('scaler', ... ),\n",
    "        ]), num_feat_names),\n",
    "        ('cat', ... , cat_feat_names)\n",
    "    ])\n",
    "\n",
    "# On applique le pipeline de préparation des données sur le jeu en s'assurant de garder les noms des variables soit retenus pour la suite\n",
    "# On utilise la fonction fit_transform() du preprocessor0\n",
    "feat_names = list(clean_dataset_feat.columns)\n",
    "X_scaled_train = preprocessor0.fit_transform(clean_dataset_feat)\n",
    "clean_feat_train_scaled = pd.DataFrame(X_scaled_train, columns=feat_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification des meilleurs hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n # Identification des meilleurs hyperparamètres #\")\n",
    "# on crée un RandomForestClassifier() avec random_state=random_state (utiliser un random_state fixe permet la reprocuctibilité des résultats)\n",
    "clf = ...\n",
    "\n",
    "# aide : tiliser la fonction identify_best_param(clf, feat, target, scorer) proposée par helping_functions.py\n",
    "best_params = ...\n",
    "best_model = RandomForestClassifier(**best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n # Cross-validation en cours #\")\n",
    "# Calcul des performances en stratified cross validation en utilisant les meilleurs hyperparametres\n",
    "\n",
    "# Configuration de cross-validation pour qu'elle soit stratifiée\n",
    "# aide : utiliser StratifiedKFold(), avec 5 découpages, shuffle=True et random_state=random_state\n",
    "stratified_cv = ...\n",
    "\n",
    "# Cross-validation\n",
    "cv_results_m0 = cross_validate(best_model,\n",
    "                               clean_feat_train_scaled,\n",
    "                               clean_dataset_target,\n",
    "                               scoring=scorers,\n",
    "                               cv=stratified_cv,\n",
    "                               return_train_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n # Performances Modèle 0 # \\n\")\n",
    "generate_performance_figure(cv_results_m0, scorers, title=\"Modèle 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Développement Modèle 1\n",
    "\n",
    "### Stratégie : Imputation par la médiane (variables numériques) et par la valeur la plus fréquente (variables catégorielles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"############ III. DEVELOPPEMENT MODELE 1 ############\")\n",
    "print(\"\\n # Preparation des données #\")\n",
    "print(\"\\n --> Gestion des données manquantes\")\n",
    "\n",
    "# Nombre d'observations et de variables présentes dans le jeu de données \n",
    "n_obs_new = len(dataset)\n",
    "n_var_new = len(dataset.columns)\n",
    "\n",
    "print(f\"\\nLe jeu de données contient {n_obs_new} observations et {n_var_new} variables.\")\n",
    "\n",
    "# Séparation des variables prédictives et de la variable cible\n",
    "dataset_feat = dataset.drop(target_feat, axis=1)\n",
    "clean_dataset_target = dataset[target_feat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline de prétraitement avec SimpleImputer\n",
    "\n",
    "Compléter le pipeline permettant de mettre à l'échelle les variables numériques avec un RobustScaler() \n",
    "puis d'imputer les données manquantes grâce à un SimpleImputer remplaçant les valeurs manquantes par la valeur médiane de la variable les variables binaire/catégorielle sont imputées en utilisant un SimpleImputer permettant de remplacer par la valeur la plus fréquente.\n",
    "\n",
    "\n",
    "aide 1 : https://machinelearningmastery.com/columntransformer-for-numerical-and-categorical-data/\n",
    "\n",
    "aide 2 : https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n",
    "\n",
    "aide 3 : https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('scaler', ...),\n",
    "            ('imputer', ...)\n",
    "        ]), num_feat_names),\n",
    "        ('cat', ..., cat_feat_names)\n",
    "    ])\n",
    "\n",
    "feat_names = list(dataset_feat.columns)\n",
    "X_scaled_train = preprocessor1.fit_transform(dataset_feat)\n",
    "clean_feat_train_scaled = pd.DataFrame(X_scaled_train, columns=feat_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification des meilleurs hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n # Identification des meilleurs hyperparamètres #\")\n",
    "# on crée un RandomForestClassifier() avec random_state=random_state (utiliser un random_state fixe permet la reprocuctibilité des résultats)\n",
    "clf = ...\n",
    "\n",
    "# aide : tiliser la fonction identify_best_param(clf, feat, target, scorer) proposée par helping_functions.py\n",
    "best_params = ...\n",
    "best_model1 = RandomForestClassifier(**best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n # Cross-validation en cours #\")\n",
    "# Calcul des performances en stratified cross validation en utilisant les meilleurs hyperparametres\n",
    "\n",
    "# Configuration de cross-validation pour qu'elle soit stratifiée\n",
    "# aide : utiliser StratifiedKFold(), avec 5 découpages, shuffle=True et random_state=random_state\n",
    "stratified_cv = ...\n",
    "\n",
    "# Cross-validation\n",
    "cv_results_m1 = cross_validate(best_model1,\n",
    "                               clean_feat_train_scaled,\n",
    "                               clean_dataset_target,\n",
    "                               scoring=scorers,\n",
    "                               cv=stratified_cv,\n",
    "                               return_train_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n # Performances Modèle 1 # \\n\")\n",
    "generate_performance_figure(cv_results_m1, scorers, title=\"Modèle 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Développement Modèle 2\n",
    "\n",
    "### Stratégie : Imputation par K-plus proches voisins (k=10) basée sur l'âge et le sexe du patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"############ III. DEVELOPPEMENT MODELE 2 ############\")\n",
    "print(\"\\n # Preparation des données #\")\n",
    "print(\"\\n --> Gestion des données manquantes\")\n",
    "\n",
    "# Nombre d'observations et de variables présentes dans le jeu de données nettoyé\n",
    "n_obs_new = len(dataset)\n",
    "n_var_new = len(dataset.columns)\n",
    "\n",
    "print(f\"\\nLe jeu de données contient {n_obs_new} observations et {n_var_new} variables.\")\n",
    "\n",
    "# Séparation des variables prédictives et de la variable cible\n",
    "dataset_feat = dataset.drop(target_feat, axis=1)\n",
    "clean_dataset_target = dataset[target_feat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline de prétraitement avec KNNImputer\n",
    "\n",
    "Compléter le pipeline permettant de mettre à l'échelle les variables numériques avec un RobustScaler() puis d'imputer les données manquantes grâce à un KNNImputer avec k=10 voisins remplaçant les valeurs manquantes par la valeur moyenne obtenus pour les k voisins les plus proches les variables binaire/catégorielle sont imputées de la même manière.\n",
    "\n",
    "aide 1 : https://machinelearningmastery.com/columntransformer-for-numerical-and-categorical-data/\n",
    "\n",
    "aide 2 : https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n",
    "\n",
    "aide 3 : https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('scaler', ...),\n",
    "            ('imputer', ...\n",
    "             )\n",
    "        ]), num_feat_names),\n",
    "        ('cat', ..., cat_feat_names)\n",
    "    ])\n",
    "\n",
    "feat_names = list(dataset_feat.columns)\n",
    "X_scaled_train = preprocessor2.fit_transform(dataset_feat)\n",
    "clean_feat_train_scaled = pd.DataFrame(X_scaled_train, columns=feat_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification des meilleurs hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n # Identification des meilleurs hyperparamètres #\")\n",
    "# on crée un RandomForestClassifier() avec random_state=random_state (utiliser un random_state fixe permet la reprocuctibilité des résultats)\n",
    "clf = ...\n",
    "\n",
    "# aide : tiliser la fonction identify_best_param(clf, feat, target, scorer) proposée par helping_functions.py\n",
    "best_params = ...\n",
    "best_model2 = RandomForestClassifier(**best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n # Cross-validation en cours #\")\n",
    "# Configuration de cross-validation pour qu'elle soit stratifiée\n",
    "# aide : utiliser StratifiedKFold(), avec 5 découpages, shuffle=True et random_state=random_state\n",
    "stratified_cv = ...\n",
    "\n",
    "# Cross-validation\n",
    "cv_results_m2 = cross_validate(best_model2,\n",
    "                               clean_feat_train_scaled,\n",
    "                               clean_dataset_target,\n",
    "                               scoring=scorers,\n",
    "                               cv=stratified_cv,\n",
    "                               return_train_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n # Performances Modèle 2 # \\n\")\n",
    "generate_performance_figure(cv_results_m2, scorers, title=\"Modèle 2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
